ğŸ¤– Smart Research Assistant â€“ LLM-Powered Multi-Tool Agent
The Smart Research Assistant is an AI-powered system designed to autonomously interpret, plan, and solve complex natural language queries by leveraging multiple specialized tools. It showcases the power of modular, tool-augmented Large Language Model (LLM) agents in real-world task orchestration, such as performing calculations, running Python code, summarizing content, and retrieving relevant information via search.

ğŸ§  What It Does
When a user inputs a multi-part natural language query (e.g., â€œSummarize this code, calculate an expression, run some Python, and search a topicâ€), the assistant:

Parses and understands the intent behind the full query.

Decomposes the query into subtasks using rule-based planning.

Selects and routes each subtask to the appropriate tool.

Executes each subtask and logs all interactions.

Returns a structured, final response combining all the outputs.

ğŸ§± Key Features
âœ… Task Planning
Breaks a compound query into discrete, logically ordered subtasks.

Determines which tool should handle each subtask (e.g., summarization, computation).

ğŸ§° Integrated Tools
Calculator: Safely evaluates math expressions (e.g., 8 \* (3 + 2)).

Code Executor: Runs lightweight Python snippets securely and returns outputs.

Summarizer: Provides concise and meaningful summaries of input text.

Search Module: Simulates fetching contextual search results for user-provided topics.

ğŸ”„ Tool Router
Dynamically routes subqueries to the correct tool based on keyword patterns.

Uses pattern matching and context parsing to extract relevant segments from the input.

ğŸ§  Memory Management
Logs every step including the original query, tool outputs, and final response.

Enables transparency and potential for future context-aware improvements.

ğŸ§ª Evaluation Suite
Allows benchmarking of the assistantâ€™s reasoning, tool usage, and accuracy across various predefined tasks.

Measures success via task-by-task verification and scores.

ğŸ› ï¸ Architecture Overview
The system is modular, comprising the following components:

bash
Copy
Edit
ai_agent_assessment/
â”œâ”€â”€ app.py # Streamlit frontend
â”œâ”€â”€ agent.py # Core agent logic (planner, router, memory)
â”œâ”€â”€ chains/
â”‚ â””â”€â”€ agent_chain.py # Orchestration controller
â”œâ”€â”€ tools/ # Tool-specific logic
â”‚ â”œâ”€â”€ calculator_tool.py
â”‚ â”œâ”€â”€ code_tool.py
â”‚ â”œâ”€â”€ summarizer_tool.py
â”‚ â””â”€â”€ search_tool.py
â”œâ”€â”€ memory/
â”‚ â””â”€â”€ memory_handler.py # Logs and short-term memory
â”œâ”€â”€ evaluation/
â”‚ â””â”€â”€ evaluator.py # Benchmarking suite
â”œâ”€â”€ logs/
â”‚ â””â”€â”€ sample_logs.txt # Interaction logs
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md # Project documentation
ğŸš€ Getting Started
Step 1: Clone and Set Up Environment
git clone https://github.com/yourusername/ai_agent_assessment.git
cd ai_agent_assessment
python -m venv venv
source venv/bin/activate # On Windows: venv\Scripts\activate
Step 2: Install Dependencies
pip install -r requirements.txt
Step 3: Run the Application
streamlit run app.py
ğŸ’¬ Example Query
Try this to see all tools in action:

Summarize this code: for i in range(3): print(i). Then calculate 12 \* (5 + 7). Also, search for common applications of LLMs.
Expected Output
ğŸ“ Summary of the code snippet.

ğŸ§® Result of the math expression: 144.

ğŸ§‘â€ğŸ’» Output of the Python snippet: printed 0 1 2.

ğŸ” Simulated search results for LLM applications.

ğŸ§ª Evaluation & Testing
Run the evaluation/evaluator.py to benchmark performance.

Evaluation Metrics
âœ”ï¸ Correctness of task planning

âœ”ï¸ Tool accuracy and reliability

âœ”ï¸ Clarity of returned outputs

âœ”ï¸ Fault tolerance and graceful error handling

Example Benchmark Task Set
Task Expected Result
Calculate 8 \* (2 + 3) 40
Summarize a paragraph Concise summary
Execute code: for i in range(3): print(i) Printed output
Search for use cases of LLMs Simulated list of sources

ğŸ“Œ Notes
Summarization and search are mocked tools for demonstration.

Safe execution environment limits scope of Python code runner.

Easily extendable for real APIs like OpenAI, SerpAPI, WolframAlpha, etc.
